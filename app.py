# -*- coding: utf-8 -*-
"""capstone_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BylrYbbIKXPAKFjbko_WiX9EXN4QgvTh
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install matplotlib
# %pip install sklearn

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
import warnings
import datetime
# Ignore warning messages
warnings.filterwarnings('ignore')

car_details = pd.read_csv("CAR DETAILS.csv")
print(type(car_details))
car_details.head(5)

car_details.shape

car_details.dtypes

duplicate = car_details.duplicated()
duplicate.sum()

car_details.describe()

df=car_details.copy()

car_details.drop_duplicates(inplace=True)
car_details.head()

car_details.shape

""" Feature engineering"""

car_details.head()

car_details.drop(['name'],axis=1,inplace=True)

null_values = car_details.isnull().sum()
null_values

car_details.head()

car_details['transmission']

""" Data Preprocessing

"""

from sklearn.impute import SimpleImputer

# Perform imputation for numerical features
imputer = SimpleImputer(strategy='median')
car_details['km_driven'] = imputer.fit_transform(car_details[['km_driven']])

# Perform imputation for categorical features
imputer = SimpleImputer(strategy='most_frequent')
car_details[['fuel', 'seller_type', 'transmission', 'owner']] = imputer.fit_transform(car_details[['fuel', 'seller_type', 'transmission', 'owner']])

car_details.head()

car_details.head()

# One-hot encoding for categorical features
car_encoded = pd.get_dummies(car_details, drop_first=True)

car_encoded

car_encoded

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Scaling the numerical features
scaler = StandardScaler()
car_encoded[['year', 'km_driven']] = scaler.fit_transform(car_encoded[['year', 'km_driven']])
car_encoded

"""Exploratory Data Analysis"""

plt.figure(figsize=(12, 8))
sns.histplot(car_details['km_driven'], kde=True)
plt.title('Distribution of Kilometers Driven')
plt.xlabel('Kilometers Driven')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(x=df['fuel'], y=car_details['selling_price'])
plt.title('Fuel Type vs Selling Price')
plt.xlabel('Fuel Type')
plt.ylabel('Selling Price')
plt.show()


plt.figure(figsize=(12, 8))
sns.boxplot(x=car_details['transmission'], y=car_details['selling_price'])
plt.title('Transmission Type vs Selling Price')
plt.xlabel('Transmission Type')
plt.ylabel('Selling Price')
plt.show()

plt.figure(figsize=(12, 8))
sns.countplot(x=car_details['seller_type'])
plt.title('Count of Seller Types')
plt.xlabel('Seller Type')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 8))
sns.countplot(x=car_details['owner'])
plt.title('Count of Owners')
plt.xlabel('Owner')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 8))
sns.scatterplot(x=df['year'], y=df['selling_price'])
plt.title('Year vs Selling Price')
plt.xlabel('Year')
plt.ylabel('Selling Price')
plt.show()

plt.figure(figsize=(12, 8))
sns.scatterplot(x=car_details['km_driven'], y=car_details['selling_price'])
plt.title('Kilometers Driven vs Selling Price')
plt.xlabel('Kilometers Driven')
plt.ylabel('Selling Price')
plt.show()

# Convert non-numeric columns to numeric if possible
car_details_numeric = car_details.apply(pd.to_numeric, errors='coerce')

# Check for NaN values and handle them
car_details_numeric.fillna(0, inplace=True)  # You can choose a different strategy for handling NaN values if necessary

# Correlation matrix
correlation_matrix = car_details_numeric.corr()
correlation_matrix



plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

from sklearn.model_selection import train_test_split

# Split the data into features and target
x = car_encoded.drop('selling_price', axis=1)
y = car_encoded['selling_price']

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2023)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

print(x.head())
print(y.head())

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.metrics import confusion_matrix, classification_report,accuracy_score
from sklearn.metrics import auc
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor

a = {'Model':[],'r2 score':[],'MSE':[],'RMSE':[],'MAE':[]}

def evaluate(model,model_name,x_test,y_test,ypred):
    mse = mean_squared_error(y_test, ypred)
    r2 = r2_score(y_test, ypred)
    mae = mean_absolute_error(y_test, ypred)
    rmse = mean_squared_error(y_test, ypred,squared=False)
    print('mean squared error',mse)
    print('r2 score',r2)
    print('mean absolute error',mae)
    print('root mean squared error',rmse)
    a['Model'].append(model_name)
    a['r2 score'].append(r2)
    a['MSE'].append(mse)
    a['RMSE'].append(rmse)
    a['MAE'].append(mae)

def m_score(model):
    print('Training Score',model.score(x_train,y_train))   # Trainig Accuracy
    print('Testing Score',model.score(x_test,y_test))      # Testing Accuracy

"""Applying Linear Regression"""

# Building the linreg model
linreg = LinearRegression()
# Fit the model on the training data
linreg.fit(x_train, y_train)

# Computing Training and testing score
m_score(linreg)

# Make predictions on the test set
ypred_linreg = linreg.predict(x_test)
print(ypred_linreg)

# Evaluate the model
evaluate(linreg,'Lin_Reg',x_test,y_test,ypred_linreg)

""" Lasso regression"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install Lasso

from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge

las = Lasso()
# Fit the model on the training data
las.fit(x_train,y_train)

# Make predictions on the test set
ypred_las = las.predict(x_test)

# Computing Training and testing score
m_score(las)

# Evaluate the model
evaluate(las,'Lasso_Reg',x_test,y_test,ypred_las)

"""Ridge regression"""

rid = Ridge()
# Fit the model on the training data
rid.fit(x_train,y_train)

# Make predictions on the test set
ypred_rid = rid.predict(x_test)

# Computing Training and testing score
m_score(rid)

evaluate(rid,'Ridge_Reg',x_test,y_test,ypred_rid)

"""Summary of regression models"""

sum = pd.DataFrame(a)
sum

"""Applying Decision Tree Regressor"""

# Create and train a decision tree classifier
dt_Regressor = DecisionTreeRegressor()
# Fit the model on the training data
dt_Regressor.fit(x_train, y_train)

# Make predictions on the test set
ypred_tree = dt_Regressor.predict(x_test)

evaluate(dt_Regressor,'Decision Tree',x_test,y_test,ypred_tree)

m_score(dt_Regressor)

"""KNN Regressor"""

knn_Regressor = KNeighborsRegressor()
# Fit the model on the training data
knn_Regressor.fit(x_train, y_train)

# Make predictions on the test set
knn_pred = knn_Regressor.predict(x_test)

evaluate(knn_Regressor,'KNN',x_test,y_test,knn_pred)

m_score(knn_Regressor)

"""Support vector Machine"""

svm_Regressor = SVR()
# Fit the model on the training data
svm_Regressor.fit(x_train, y_train)

# Make predictions on the test set
svm_pred = svm_Regressor.predict(x_test)

evaluate(svm_Regressor,'SVM',x_test,y_test,svm_pred)

m_score(svm_Regressor)

"""Random forest"""

# Building the RF Model
rf = RandomForestRegressor(n_estimators=80,min_samples_split=50,max_depth=8)
# Fit the model on the training data
rf.fit(x_train,y_train)

ypred_rf=rf.predict(x_test)

evaluate(rf,'Random Forest',x_test,y_test,ypred_rf)

m_score(rf)

"""Comparing different Classification Models"""

Model_performance = pd.DataFrame(a)
Model_performance

print(x_train.shape)
print(x_test.shape)
print(x.shape)
print(y.shape)

# Building the RF Model
Bestmodel = RandomForestRegressor(n_estimators=80,min_samples_split=50,max_depth=8)
# Fit the model on the Actual data
Bestmodel.fit(x,y)

car_encoded.head()

#Predict on New Data
#Create a new dataset with 20 random data points from the car details dataset
new_data = car_encoded.sample(n=20, random_state=2023)
new_data.shape

new_data['selling_price'].value_counts()

new_data.to_csv('sample_major.csv')

# Assuming the saved model is loaded as 'model'
X_test = new_data.drop("selling_price", axis=1)  # Assuming "selling_price" is the target variable
Y_test = new_data["selling_price"]

"""Saving and Loading the Best Model:"""

import joblib
# Save the best model
joblib.dump(Bestmodel, 'best_model.joblib')
# Load the model
loaded_model = joblib.load('best_model.joblib')

# Make predictions using the loaded model
predictions = loaded_model.predict(X_test)

print('Predictions:', predictions)

results = pd.DataFrame({"Actual Selling Price": Y_test, "Predicted Selling Price": predictions})
print(results)
